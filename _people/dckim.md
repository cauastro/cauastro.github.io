---
name: Dongchan Kim
position: undergrad
avatar: dck.jpg
twitter:
joined: 2023
---

<img width="300" src="{{site.baseurl}}/images/people/{{page.avatar}}" data-action="zoom">

### Contact

<!-- [ari-benjamin.com](http://ari-benjamin.com)<br> -->
<i class="fa fa-envelope-o"></i>  `krq3268@cau.ac.kr`<br>
<!-- <i class="fa fa-bar-chart"></i> [google scholar](https://scholar.google.com/citations?user=GW6D4ZIAAAAJ&hl=en) <br> -->

<hr>

### Research Interests

Numerical Relativity

### Bio

After graduating from Williams College with a degreee in physics, teaching high school chemistry for a year in Mexico, and obtaining a master's in nanoscale simulation and biomaterials at Northwestern, I joined this awesome lab for computational neuroscience! I'm all about nonlinear life trajectories, which is fair, since the brain is nonlinear too. 

- _Penn Integrated Knowledge Professor, University of Pennsylvania_<br>
- _Science coach. Collaborator. Transdisciplinary optimist._

<i class="fa fa-envelope-o"></i> `kording@upenn.edu`

**Office**<br>
404B Richards, 3700 Hamilton Walk <br>
Philadelphia, PA 19104

[Konrad Kording](http://koerding.com/) runs his lab at the University of Pennsylvania.
Konrad is interested in the question of how the brain solves the credit assignment problem and
similarly how we should assign credit in the real world (through causality). In extension
of this main thrust he is interested in applications of causality in biomedical research.
Konrad has trained as student at ETH Zurich with Peter Konig, as postdoc at UCL London with Daniel
Wolpert and at MIT with Josh Tenenbaum. After a decade at Northwestern University he is now
PIK professor at UPenn.

### Research Interests

I'm working on a variety of topics, loosely collected under the heading of **finding or creating useful structure in neural representations**

- how can we better understand the layer-wise steps that neural networks take transforming inputs to ouputs?
- what are the advantages of a "modular" neural network, and how do we build them?
- how does the brain learn and model causal relationships?

And growing out of my earlier work on Bayesian inference in the brain, a separate line of my recent work asks

- what sort of approximate inference algorithms exist "between" MCMC and variational inference?
- (how) does the brain represent probability?

### Bio

My undergrad was at Dartmouth College, where I mostly did Computer Science and Engineering, but sparked an interest in the connection between AI and neuroscience. This led me in 2014 to a PhD program in Computer Science at the University of Rochester, where I quickly discovered that making "brain inspired AI" means first understanding "brains." I transferred to the Brain and Cognitive Science department in 2015, where I did my main PhD work on Bayesian Inference in low-level visual perception, graduating in fall 2020.

By the end of my PhD, I saw some serious flaws in the Bayesian framework as a tool for understanding (and building) neural computation. This has led down three paths in my postdoc work:

- expanding what is meant by "Bayesian inference" by developing new algorithms
- diving into the philosophy of what "neural computation" is in the first place
- turning my attention to deep learning and asking what kind of useful structure is there, or could be built in

### Links

<i class="fa fa-bar-chart-o" /> [Google Scholar](https://scholar.google.com/citations?user=xc-Z4CoAAAAJ) <br />
<i class="fa fa-wordpress" /> [Blog](https://boxandarrowbrain.com) <br />
<i class="fa fa-newspaper-o" /> [CV (pdf)](/documents/RDL_CV_20F.pdf) <br />
<i class="fa fa-envelope-o" /> `[lastname].[firstname].d@gmail.com` <br />
