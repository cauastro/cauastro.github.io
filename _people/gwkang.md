---
name: Gungwon Kang
position: pi
avatar: gwk.png
twitter:
joined: 2022
---

<img width="200" src="{{site.baseurl}}/images/people/{{page.avatar}}" data-action="zoom">

### Contact

<!-- [ari-benjamin.com](http://ari-benjamin.com)<br> -->
<i class="fa fa-envelope-o"></i>  `gwkang@cau.ac.kr`<br>
<!-- <i class="fa fa-bar-chart"></i> [google scholar](https://scholar.google.com/citations?user=GW6D4ZIAAAAJ&hl=en) <br> -->

**Office**<br>
Rm. 201 at Bld. # 209, Chung-Ang University<br>
<hr>

### Education
- 1990/08 ~ 1995/08 University of Maryland 물리학과 박사(일반상대론 전공)
- 1982/03 ~ 1988/02 연세대학교 물리학과 학사, 석사(물리학 전공)

### Career

- 2022/02 ~ 현재 중앙대학교 물리학과 교수
- 2009/09 ~ 현재 한국중력파연구협력단 멤버, LSC(LIGO Scientific Collaboration) 카운실 멤버
- 2021/02 ~ 2022/02 중앙대학교 미래융합원 고에너지물리센터 교수
- 2016/01 ~ 2017/12 한국물리학회 천체물리분과위원장
- 2007/09 ~ 2021/08 KAIST 물리학과 겸직교수
- 2005/06 ~ 2021/01 한국과학기술정보연구원(KISTI) 책임연구원
- 2000/09 ~ 2002/08 KEK JSPS Fellow, 2002/09 ~ 2005/05 한국고등과학원(KIAS) 연구원
- 1995/09 ~ 2000/08 Raman Research Institute(2), 서강대(1), 연세대(1) 박사후 연구원, 인제대 연구교수(1)


### Research Interests

중력 및 천체물리(Gravitation and Astrophysics)

1. Gravitational Waves and Numerical Relativity
- Numerical simulations for black hole coalescences and encounters
- Gravitational wave-form modeling
- Multi-messenger astrophysics

2. Black Holes and Quantum Gravity
- Stability of black string/brane configurations
- Black hole entropy, Black hole thermodynamics in general
- Spacetime singularity

3. Cosmology
- Early universe: origin of the initial singularity and fluctuations

<!-- 
### Bio

After graduating from Williams College with a degreee in physics, teaching high school chemistry for a year in Mexico, and obtaining a master's in nanoscale simulation and biomaterials at Northwestern, I joined this awesome lab for computational neuroscience! I'm all about nonlinear life trajectories, which is fair, since the brain is nonlinear too. 

- _Penn Integrated Knowledge Professor, University of Pennsylvania_<br>
- _Science coach. Collaborator. Transdisciplinary optimist._

<i class="fa fa-envelope-o"></i> `kording@upenn.edu`

**Office**<br>
404B Richards, 3700 Hamilton Walk <br>
Philadelphia, PA 19104

[Konrad Kording](http://koerding.com/) runs his lab at the University of Pennsylvania.
Konrad is interested in the question of how the brain solves the credit assignment problem and
similarly how we should assign credit in the real world (through causality). In extension
of this main thrust he is interested in applications of causality in biomedical research.
Konrad has trained as student at ETH Zurich with Peter Konig, as postdoc at UCL London with Daniel
Wolpert and at MIT with Josh Tenenbaum. After a decade at Northwestern University he is now
PIK professor at UPenn.

### Research Interests

I'm working on a variety of topics, loosely collected under the heading of **finding or creating useful structure in neural representations**

- how can we better understand the layer-wise steps that neural networks take transforming inputs to ouputs?
- what are the advantages of a "modular" neural network, and how do we build them?
- how does the brain learn and model causal relationships?

And growing out of my earlier work on Bayesian inference in the brain, a separate line of my recent work asks

- what sort of approximate inference algorithms exist "between" MCMC and variational inference?
- (how) does the brain represent probability?

### Bio

My undergrad was at Dartmouth College, where I mostly did Computer Science and Engineering, but sparked an interest in the connection between AI and neuroscience. This led me in 2014 to a PhD program in Computer Science at the University of Rochester, where I quickly discovered that making "brain inspired AI" means first understanding "brains." I transferred to the Brain and Cognitive Science department in 2015, where I did my main PhD work on Bayesian Inference in low-level visual perception, graduating in fall 2020.

By the end of my PhD, I saw some serious flaws in the Bayesian framework as a tool for understanding (and building) neural computation. This has led down three paths in my postdoc work:

- expanding what is meant by "Bayesian inference" by developing new algorithms
- diving into the philosophy of what "neural computation" is in the first place
- turning my attention to deep learning and asking what kind of useful structure is there, or could be built in

### Links

<i class="fa fa-bar-chart-o" /> [Google Scholar](https://scholar.google.com/citations?user=xc-Z4CoAAAAJ) <br />
<i class="fa fa-wordpress" /> [Blog](https://boxandarrowbrain.com) <br />
<i class="fa fa-newspaper-o" /> [CV (pdf)](/documents/RDL_CV_20F.pdf) <br />
<i class="fa fa-envelope-o" /> `[lastname].[firstname].d@gmail.com` <br /> -->
